(window.webpackJsonp=window.webpackJsonp||[]).push([[68],{448:function(n,e,r){"use strict";r.r(e);var a=r(42),s=Object(a.a)({},(function(){var n=this,e=n.$createElement,r=n._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[r("h2",{attrs:{id:"arts-2019-左耳听风社群活动-每周完成一个-arts"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#arts-2019-左耳听风社群活动-每周完成一个-arts"}},[n._v("#")]),n._v(" ARTS-2019 左耳听风社群活动--每周完成一个 ARTS")]),n._v(" "),r("p",[n._v("1.Algorithm： 每周至少做一个 leetcode 的算法题\n2.Review: 阅读并点评至少一篇英文技术文章\n3.Tip: 学习至少一个技术技巧\n4.Share: 分享一篇有观点和思考的技术文章")]),n._v(" "),r("h3",{attrs:{id:"_1-algorithm"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-algorithm"}},[n._v("#")]),n._v(" 1.Algorithm:")]),n._v(" "),r("p",[n._v("Trapping Rain Water https://leetcode.com/submissions/detail/401829300/")]),n._v(" "),r("h3",{attrs:{id:"_2-review"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2-review"}},[n._v("#")]),n._v(" 2.Review:")]),n._v(" "),r("p",[n._v("https://towardsdatascience.com/data-engineering-how-to-set-dependencies-between-data-pipelines-in-apache-airflow-using-sensors-fc34cfa55fba\n如何在 Apache Airflow 中设置数据管道之间的依赖关系")]),n._v(" "),r("h4",{attrs:{id:"点评："}},[r("a",{staticClass:"header-anchor",attrs:{href:"#点评："}},[n._v("#")]),n._v(" 点评：")]),n._v(" "),r("p",[n._v("作者 Nicholas Leong 提出作为数据工程师，能做的最好的事情就是建立有效的管道，以适应数据仓库本身的优点和缺点。")]),n._v(" "),r("ul",[r("li",[n._v("每个数据仓库都从数据源获取数据，并且可能是主数据库或该数据库的副本。我们认为运行ELT管道会更快，更省钱，因为它减轻了主数据库从服务器的负担。是目前在数据领域中非常普遍的做法。通常的议程是纯数据提取的原始表从午夜开始X个小时，导致这些表再转换X个小时以完成整个管道。")]),n._v(" "),r("li",[n._v("如此设计管道的原因是因为已转换表的第一级产生并取决于原始表提取的完成。然后，第二级转换表依赖于这些第一级转换表。因此，重要的是我们设置这些任务之间的依赖关系。如果尚未提取/转换原始表之一，则我们不希望执行该表的转换。这将导致数据不正确，这正是数据工程师应负的责任。")])]),n._v(" "),r("p",[n._v("传感器")]),n._v(" "),r("ul",[r("li",[n._v("Apache Airflow 作为我们的主要任务流管理。这样，我们可以有效地监视我们的工作流程，能够跟踪失败的任务。传感器预先内置在 Airflow 中。监控 Airflow 任何任务的完成状态就这么简单。我们将使用传感器设置 DAGS 管道之间的依赖关系，以便在依赖关系完成之前不会运行。无需为此编写任何自定义运算符。")]),n._v(" "),r("li",[n._v("执行增量可能很棘手，它取决于任务的执行时间，而不是运行时间。")]),n._v(" "),r("li",[n._v("超时参数是必需的。当管道扩展时，将运行许多传感器以检查完工情况。如果未设置超时且我们的某些依存关系失败，则传感器将无限期运行，并导致 Airflow 停止。这是因为 Airflow 仅允许在实例上运行一定数量的最大任务，并且传感器被视为任务。如果以某种方式达到该数字，Airflow 将不再处理其他任务。")])]),n._v(" "),r("p",[n._v("结论：\n传感器可用于所有 Airflow 任务。想在查询运行后发送电子邮件吗？使用传感器")]),n._v(" "),r("h3",{attrs:{id:"_3-tip"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3-tip"}},[n._v("#")]),n._v(" 3.Tip:")]),n._v(" "),r("p",[n._v("使用 docker 部署 airflow")]),n._v(" "),r("p",[n._v("1.localExcutor 适用于那种不需要做横向扩展的情况,一般就是单机部署,它的好处是相对更加轻量,对资源的消耗更低,也不需要依赖消息队列.")]),n._v(" "),r("div",{staticClass:"language-xml extra-class"},[r("pre",{pre:!0,attrs:{class:"language-xml"}},[r("code",[n._v('version: \'3.6\'\nservices:\n  postgres:\n    image: postgres:latest\n    environment:\n      - POSTGRES_USER=airflow\n      - POSTGRES_PASSWORD=airflow\n      - POSTGRES_DB=airflow\n\n  webserver:\n    image: puckel/docker-airflow:latest\n    restart: always\n    depends_on:\n      - postgres\n    environment:\n      - LOAD_EX=n\n      - EXECUTOR=Local\n    volumes:\n      - ./dags:/usr/local/airflow/dags\n      # Uncomment to include custom plugins\n      # - ./plugins:/usr/local/airflow/plugins\n    ports:\n      - 8080:8080\n    command: webserver\n    healthcheck:\n      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]\n      interval: 30s\n      timeout: 30s\n      retries: 3\n')])])]),r("p",[n._v("2.CeleryExcutor 可以借助消息队列实现横向扩展,适合有大量错综复杂任务流,且必须使用集群的的情况.")]),n._v(" "),r("div",{staticClass:"language-xml extra-class"},[r("pre",{pre:!0,attrs:{class:"language-xml"}},[r("code",[n._v('version: \'3.6\'\nservices:\n  redis: #如果有外部的redis则可以不创建\n    image: redis:latest\n  broker: #如果有外部的rabbitmq则可以不创建\n    image: rabbitmq:3.7-management\n\n  postgres: #如果有外部的pg则可以不创建\n    image: postgres:latest\n    environment:\n        - POSTGRES_USER=airflow\n        - POSTGRES_PASSWORD=airflow\n        - POSTGRES_DB=airflow\n\n  airflow-webserver:\n    image: puckel/docker-airflow:latest\n    restart: always\n    depends_on:\n      - postgres\n      - broker\n      - redis\n    environment:\n      - LOAD_EX=n\n      - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=\n      - EXECUTOR=Celery\n      - AIRFLOW__CELERY__BROKER_URL=amqp://guest:guest@broker:5672/\n      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:3306/airflow\n      # - POSTGRES_USER=airflow\n      # - POSTGRES_PASSWORD=airflow\n      # - POSTGRES_DB=airflow\n      # - REDIS_PASSWORD=redispass\n    volumes:\n      - dags:/usr/local/airflow/dags\n      # Uncomment to include custom plugins\n      # - ./plugins:/usr/local/airflow/plugins\n    ports:\n      - 8080:8080\n    networks:\n      - net-output\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n    command: webserver\n    healthcheck:\n      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]\n      interval: 30s\n      timeout: 30s\n      retries: 3\n\n  flower:\n    image: puckel/docker-airflow:latest\n    restart: always\n    depends_on:\n      - broker\n      - redis\n    environment:\n      - EXECUTOR=Celery\n      - AIRFLOW__CELERY__BROKER_URL=amqp://guest:guest@broker:5672/\n      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:3306/airflow\n      # - REDIS_PASSWORD=redispass\n    networks:\n      - net-output\n    deploy:\n      replicas: 1\n      # restart_policy:\n      #   condition: on-failure\n    command: flower\n\n  scheduler:\n    image: puckel/docker-airflow:latest\n    restart: always\n    depends_on:\n      - airflow-webserver\n    volumes:\n      - dags:/usr/local/airflow/dags\n      # Uncomment to include custom plugins\n      # - ./plugins:/usr/local/airflow/plugins\n    environment:\n      - LOAD_EX=n\n      - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=\n      - EXECUTOR=Celery\n      - AIRFLOW__CELERY__BROKER_URL=amqp://guest:guest@broker:5672/\n      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:3306/airflow\n      # - POSTGRES_USER=airflow\n      # - POSTGRES_PASSWORD=airflow\n      # - POSTGRES_DB=airflow\n      # - REDIS_PASSWORD=redispass\n    deploy:\n      replicas: 1\n      # restart_policy:\n      #   condition: on-failure\n    command: scheduler\n\n  worker:\n    image: puckel/docker-airflow:latest\n    restart: always\n    depends_on:\n      - scheduler\n    volumes:\n      - dags:/usr/local/airflow/dags\n      # Uncomment to include custom plugins\n      # - ./plugins:/usr/local/airflow/plugins\n    environment:\n      - FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=\n      - EXECUTOR=Celery\n      - AIRFLOW__CELERY__BROKER_URL=amqp://guest:guest@broker:5672/\n      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:3306/airflow\n      # - POSTGRES_USER=airflow\n      # - POSTGRES_PASSWORD=airflow\n      # - POSTGRES_DB=airflow\n      # - REDIS_PASSWORD=redispass\n    deploy:\n      replicas: 4\n      # restart_policy:\n      #   condition: on-failure\n    command: worker\n\n  networks:\n    net-output:\n      external: true\n  volumes:\n    dags:\n      driver_opts:\n      type: "nfs"\n      o: "addr=10.40.0.199,nolock,soft,rw"\n      device: ":/docker/dags"\n')])])]),r("h3",{attrs:{id:"_4-share"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-share"}},[n._v("#")]),n._v(" 4.Share:")]),n._v(" "),r("p",[n._v("https://zhuanlan.zhihu.com/p/43383509\nairflow 实战总结")])])}),[],!1,null,null,null);e.default=s.exports}}]);