(window.webpackJsonp=window.webpackJsonp||[]).push([[77],{458:function(t,s,a){"use strict";a.r(s);var n=a(42),r=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"arts-2019-左耳听风社群活动-每周完成一个-arts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#arts-2019-左耳听风社群活动-每周完成一个-arts"}},[t._v("#")]),t._v(" ARTS-2019 左耳听风社群活动--每周完成一个 ARTS")]),t._v(" "),a("p",[t._v("1.Algorithm： 每周至少做一个 leetcode 的算法题\n2.Review: 阅读并点评至少一篇英文技术文章\n3.Tip: 学习至少一个技术技巧\n4.Share: 分享一篇有观点和思考的技术文章")]),t._v(" "),a("h3",{attrs:{id:"_1-algorithm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-algorithm"}},[t._v("#")]),t._v(" 1.Algorithm:")]),t._v(" "),a("ol",{attrs:{start:"215"}},[a("li",[t._v("数组中的第K个最大元素 https://leetcode-cn.com/submissions/detail/127168639/\n剑指 Offer 40. 最小的k个数 https://leetcode-cn.com/submissions/detail/127168560/\n面试题 17.14. 最小K个数 https://leetcode-cn.com/submissions/detail/127168482/")]),t._v(" "),a("li",[t._v("二分查找 https://leetcode-cn.com/submissions/detail/127156061/")])]),t._v(" "),a("h3",{attrs:{id:"_2-review"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-review"}},[t._v("#")]),t._v(" 2.Review:")]),t._v(" "),a("p",[t._v("http://bicortex.com/data-analysis-with-dask-a-python-scale-out-parallel-computation-framework-for-big-data/\n使用 Dask 进行数据分析–适用于大数据的 Python 横向扩展并行计算框架")]),t._v(" "),a("h4",{attrs:{id:"点评："}},[a("a",{staticClass:"header-anchor",attrs:{href:"#点评："}},[t._v("#")]),t._v(" 点评：")]),t._v(" "),a("p",[t._v("作者目睹越来越多的人们致力于数据领域的研究，他们倾向于使用Python丰富的库生态系统，尤其是所谓的Python开放数据科学堆栈，即Pandas，NumPy，SciPy和Scikit-learn，远离诸如Fortran，Matlab和Octave之类的行业巨头。")]),t._v(" "),a("p",[t._v("对于相对较小的数据集而言，使用 Python 开放数据科学堆栈，并且可以轻松地放入RAM中。这样一来，就可以轻松地产生，收集，存储和处理比以前更多的数据，而价格却一直在下降。")]),t._v(" "),a("ul",[a("li",[t._v("Pandas 等工具进行数据清理和探索性数据分析")]),t._v(" "),a("li",[t._v("SciPy 和 NumPy 对数据进行统计测试")]),t._v(" "),a("li",[t._v("Scikit-Learn 建立预测模型。")])]),t._v(" "),a("p",[t._v("但是，由于数据收集和存储费用的减少，数据科学家越来越多地致力于解决涉及分析大量数据集的问题。当使用超过一定大小的数据集时，这些工具对其可行性具有上限。由于它们本身并不是为在分布式数据集上运行而构建的，一旦超过阈值，由于痛苦的长时间运行，即使是最简单的计算，不稳定的代码和笨拙的工作流，也很难从数据中提取含义。大型数据集既不能放入RAM，也不能放入一台计算机的永久性存储中。这些数据集的大小通常超过1 TB，并且根据问题的不同，可以达到PB甚至更高。")]),t._v(" "),a("p",[t._v("Dask 由 2014 年下半年由 Matthew Rocklin 发起项目，目标是将原生可扩展性带入 Python 开放数据科学堆栈，并克服其单机限制。Dask由几个不同的组件和API组成，可以分为三层：任务计划程序，低级API和高级别API。")]),t._v(" "),a("ul",[a("li",[t._v("任务调度程序是核心，它可以协调和监视 CPU 内核和计算机之间的计算执行。这些计算在代码中表示为 Dask Delayed 对象或 Dask Futures 对象（主要区别在于前者是惰性计算的-意味着它们在需要值时会及时评估，而后者则是热切评估-意味着它们实时评估，无论是否立即需要该值）。Dask 的高级 PI 为 Delayed 和 Futures 对象提供了一层抽象。这些高级对象上的操作会导致由任务调度程序管理许多并行的低级操作，从而为用户提供了无缝的体验。")]),t._v(" "),a("li",[t._v("Dask 的 API 集来解决广泛范围内的问题，例如多维数据分析，可扩展的机器学习训练以及对大型模型的预测等，其主要用途之一就是能够实现类似 Pandas 的工作流程，即在大数据上按时间序列，商业智能和常规数据处理启用应用程序。与 Pandas 一样，Dask 也利用 DataFrame 的概念，大多数功能与 Pandas 重叠。Dask DataFrame 是由许多较小的 Pandas DataFrame 组成的大型并行 DataFrame，它们沿索引分割。这些 Pandas DataFrame 可以驻留在磁盘上，以便在一台计算机上或群集中的许多不同计算机上进行大于内存的计算。一个 Dask DataFrame 操作触发对组成的 Pandas DataFrames 的许多操作。")])]),t._v(" "),a("p",[t._v("总结，在 Pandas 上使用类似 Dask 数据帧这样的并行数据帧的性能优势（或缺点）根据执行的计算类型而有所不同：")]),t._v(" "),a("ul",[a("li",[t._v("如果要进行小型计算，那么 Pandas 始终是正确的选择。并行化的管理成本将超过任何收益。如果计算花费不到100毫秒，则不应并行化。")]),t._v(" "),a("li",[t._v("对于诸如过滤，清理和聚合大数据之类的简单操作，应该通过使用并行数据帧实现线性加速。如果使用的是20核计算机，则可能会获得20倍的加速。随着扩展，管理开销将增加，因此加速会有所降低。")]),t._v(" "),a("li",[t._v("对于像分布式联接这样的复杂操作，它会更加复杂。可能会得到如上所述的线性加速，甚至可能会减速。具有类似数据库的计算和并行计算经验的人可能会很好地预测哪些计算会做得很好。")])]),t._v(" "),a("h3",{attrs:{id:"_3-tip"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-tip"}},[t._v("#")]),t._v(" 3.Tip:")]),t._v(" "),a("ol",[a("li",[t._v("Python 字符串转换为数值")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#整数")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'123'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("123")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1.23'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nValueError\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nValueError\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nTypeError\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#浮点数")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'123'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("123.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1.23'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.23")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nValueError\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nTypeError\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1e3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000.0")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#百分号的数值")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" s"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'12%'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rstrip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.12")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#中文字符的数值字符")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" unicodedata\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" unicodedata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numeric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'三'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" unicodedata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numeric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'二十一'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nTypeError"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" numeric"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" argument "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" must be a "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("unicode")]),t._v(" character"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),t._v("\n\n")])])]),a("ol",{attrs:{start:"2"}},[a("li",[t._v("用 pandas 处理大型 csv 文件")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1.读取限定列")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'demo.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("usecols"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.读取限定行")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'demo.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("nrows"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("usecols"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.分块读取")]),t._v("\nreader "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'demo.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("nrows"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     usecols"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column1'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column2'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'column3'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                     chunksize"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("iterator"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nreader\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4.head()方法默认是10条，也可以用tail()方法查看最后10条数据。")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'demo.csv'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tail"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),a("ol",{attrs:{start:"3"}},[a("li",[t._v("Elasticsearch 任务管理 api")])]),t._v(" "),a("div",{staticClass:"language-json extra-class"},[a("pre",{pre:!0,attrs:{class:"language-json"}},[a("code",[t._v("#任务api会从一个节点或集群中所有节点获取任务列表及状态.\nGET /_tasks \nGET /_tasks?nodes=nodeId1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("nodeId2 \nGET /_tasks?nodes=nodeId1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("nodeId2&actions=cluster"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("* \n注意：\nReindex 不会复制源索引的设置，应在执行 reindex 操作之前，提前设置好目标索引的映射、分片数等。\n\n#任务id查询获取任务\nGET /_tasks/taskId1\nGET /_tasks?parent_task_id=parentTaskId1\n\n#指定某个任务的状态一直等待该任务完成或者满足超时的条件\nGET /_tasks/tidxiaorui.cctid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12345")]),t._v("?wait_for_completion="),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),t._v("&timeout=10s\n\n#强制取消暂停某个任务\nPOST /_tasks/taskId1/_cancel\n\n#同时取消多个任务\nPOST /_tasks/_cancel?node_id=nodeId1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("nodeId2&actions=*reindex\n\n")])])]),a("h3",{attrs:{id:"_4-share"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-share"}},[t._v("#")]),t._v(" 4.Share:")]),t._v(" "),a("p",[t._v("https://zhuanlan.zhihu.com/p/100535193\n为什么选择R而不是Python做ETL")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/xinlingjun2007/article/details/80358033\nGPU数据库介绍")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/vkingnew/article/details/85339844\nGPU 数据库")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/wushijingzuo/article/details/109554839\n新一代Notebook神器出现，Jupyter危险了！")])])}),[],!1,null,null,null);s.default=r.exports}}]);